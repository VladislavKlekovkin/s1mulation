{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-cross-entropy-method.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMJ10cLzT+6acYpXRCQtqe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VladislavKlekovkin/s1mulation/blob/master/1_cross_entropy_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWpKIaphlWLn"
      },
      "source": [
        "You need to run this code from the google colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Qz52sS6gvt",
        "outputId": "eb31961e-9dd5-449e-ea34-b5654179bc58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "!git clone https://github.com/VladislavKlekovkin/s1mulation.git \n",
        "sys.path.append(os.path.join('./', 's1mulation'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 's1mulation' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUqiFWDbgqLd"
      },
      "source": [
        "from environment import Space\n",
        "\n",
        "#import gym\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "\n",
        "WIDTH = 8\n",
        "HEIGHT = 8\n",
        "\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "PERCENTILE = 70"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoJkK3Mnw2oK"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsnFERQSw5cF"
      },
      "source": [
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb2MiwFZw8eB"
      },
      "source": [
        "def iterate_batches(env, net, batch_size):\n",
        "    batch = []\n",
        "    episode_reward = 0.0\n",
        "    episode_steps = []\n",
        "    obs = env.reset()\n",
        "    sm = nn.Softmax(dim=1)\n",
        "    while True:\n",
        "        obs_v = torch.FloatTensor([obs])\n",
        "        act_probs_v = sm(net(obs_v))\n",
        "        act_probs = act_probs_v.data.numpy()[0]\n",
        "        action = np.random.choice(len(act_probs), p=act_probs)\n",
        "        next_obs, reward, is_done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
        "        if is_done:\n",
        "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
        "            episode_reward = 0.0\n",
        "            episode_steps = []\n",
        "            next_obs = env.reset()\n",
        "            if len(batch) == batch_size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "        obs = next_obs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl5Y2y3Sw__x"
      },
      "source": [
        "def filter_batch(batch, percentile):\n",
        "    rewards = list(map(lambda s: s.reward, batch))\n",
        "    reward_bound = np.percentile(rewards, percentile)\n",
        "    reward_mean = float(np.mean(rewards))\n",
        "\n",
        "    train_obs = []\n",
        "    train_act = []\n",
        "    for example in batch:\n",
        "        if example.reward < reward_bound:\n",
        "            continue\n",
        "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
        "        train_act.extend(map(lambda step: step.action, example.steps))\n",
        "\n",
        "    train_obs_v = torch.FloatTensor(train_obs)\n",
        "    train_act_v = torch.LongTensor(train_act)\n",
        "    return train_obs_v, train_act_v, reward_bound, reward_mean"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSI_5le3lKmi",
        "outputId": "eba898cb-e823-49ac-c801-1c5796da0249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env = Space(width=WIDTH, height=HEIGHT)\n",
        "# env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
        "obs_size = env.width * env.height\n",
        "n_actions = len(env.action_space)\n",
        "\n",
        "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "writer = SummaryWriter(comment=\"-cartpole\")\n",
        "\n",
        "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
        "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
        "    optimizer.zero_grad()\n",
        "    action_scores_v = net(obs_v)\n",
        "    loss_v = objective(action_scores_v, acts_v)\n",
        "    loss_v.backward()\n",
        "    optimizer.step()\n",
        "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
        "        iter_no, loss_v.item(), reward_m, reward_b))\n",
        "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
        "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
        "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
        "    if reward_m > 0.9:\n",
        "        print(\"Solved!\")\n",
        "        break\n",
        "writer.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evn initializing\n",
            "0: loss=1.389, reward_mean=0.1, reward_bound=0.1\n",
            "1: loss=1.386, reward_mean=0.1, reward_bound=0.2\n",
            "2: loss=1.377, reward_mean=0.1, reward_bound=0.2\n",
            "3: loss=1.362, reward_mean=0.2, reward_bound=0.2\n",
            "4: loss=1.357, reward_mean=0.2, reward_bound=0.2\n",
            "5: loss=1.320, reward_mean=0.2, reward_bound=0.2\n",
            "6: loss=1.294, reward_mean=0.2, reward_bound=0.3\n",
            "7: loss=1.293, reward_mean=0.2, reward_bound=0.2\n",
            "8: loss=1.233, reward_mean=0.2, reward_bound=0.3\n",
            "9: loss=1.202, reward_mean=0.3, reward_bound=0.5\n",
            "10: loss=1.181, reward_mean=0.2, reward_bound=0.3\n",
            "11: loss=1.216, reward_mean=0.2, reward_bound=0.2\n",
            "12: loss=1.167, reward_mean=0.2, reward_bound=0.1\n",
            "13: loss=1.131, reward_mean=0.2, reward_bound=0.0\n",
            "14: loss=1.091, reward_mean=0.3, reward_bound=0.4\n",
            "15: loss=1.142, reward_mean=0.2, reward_bound=0.2\n",
            "16: loss=1.114, reward_mean=0.2, reward_bound=0.0\n",
            "17: loss=1.099, reward_mean=0.3, reward_bound=0.4\n",
            "18: loss=1.091, reward_mean=0.2, reward_bound=0.0\n",
            "19: loss=1.153, reward_mean=0.2, reward_bound=0.4\n",
            "20: loss=1.179, reward_mean=0.2, reward_bound=0.1\n",
            "21: loss=1.079, reward_mean=0.2, reward_bound=0.2\n",
            "22: loss=1.101, reward_mean=0.3, reward_bound=0.4\n",
            "23: loss=1.145, reward_mean=0.2, reward_bound=0.0\n",
            "24: loss=1.137, reward_mean=0.2, reward_bound=0.3\n",
            "25: loss=1.035, reward_mean=0.3, reward_bound=0.5\n",
            "26: loss=1.148, reward_mean=0.2, reward_bound=0.0\n",
            "27: loss=1.122, reward_mean=0.2, reward_bound=0.4\n",
            "28: loss=1.087, reward_mean=0.3, reward_bound=0.5\n",
            "29: loss=1.094, reward_mean=0.3, reward_bound=0.4\n",
            "30: loss=1.179, reward_mean=0.2, reward_bound=0.1\n",
            "31: loss=1.114, reward_mean=0.3, reward_bound=0.5\n",
            "32: loss=1.017, reward_mean=0.3, reward_bound=0.6\n",
            "33: loss=1.030, reward_mean=0.2, reward_bound=0.3\n",
            "34: loss=1.048, reward_mean=0.3, reward_bound=0.4\n",
            "35: loss=1.106, reward_mean=0.2, reward_bound=0.3\n",
            "36: loss=1.034, reward_mean=0.3, reward_bound=0.5\n",
            "37: loss=0.982, reward_mean=0.3, reward_bound=0.5\n",
            "38: loss=0.921, reward_mean=0.3, reward_bound=0.6\n",
            "39: loss=0.892, reward_mean=0.3, reward_bound=0.7\n",
            "40: loss=0.756, reward_mean=0.3, reward_bound=0.7\n",
            "41: loss=0.825, reward_mean=0.3, reward_bound=0.7\n",
            "42: loss=0.895, reward_mean=0.2, reward_bound=0.0\n",
            "43: loss=0.761, reward_mean=0.3, reward_bound=0.6\n",
            "44: loss=0.808, reward_mean=0.3, reward_bound=0.0\n",
            "45: loss=0.781, reward_mean=0.2, reward_bound=0.0\n",
            "46: loss=0.783, reward_mean=0.3, reward_bound=0.6\n",
            "47: loss=0.785, reward_mean=0.3, reward_bound=0.8\n",
            "48: loss=0.722, reward_mean=0.3, reward_bound=0.0\n",
            "49: loss=0.639, reward_mean=0.3, reward_bound=0.6\n",
            "50: loss=0.637, reward_mean=0.3, reward_bound=0.1\n",
            "51: loss=0.552, reward_mean=0.3, reward_bound=0.0\n",
            "52: loss=0.597, reward_mean=0.3, reward_bound=0.7\n",
            "53: loss=0.637, reward_mean=0.3, reward_bound=0.7\n",
            "54: loss=0.664, reward_mean=0.2, reward_bound=0.0\n",
            "55: loss=0.497, reward_mean=0.4, reward_bound=0.8\n",
            "56: loss=0.481, reward_mean=0.3, reward_bound=0.7\n",
            "57: loss=0.669, reward_mean=0.2, reward_bound=0.0\n",
            "58: loss=0.381, reward_mean=0.4, reward_bound=1.0\n",
            "59: loss=0.514, reward_mean=0.4, reward_bound=1.0\n",
            "60: loss=0.525, reward_mean=0.3, reward_bound=0.8\n",
            "61: loss=0.398, reward_mean=0.4, reward_bound=1.0\n",
            "62: loss=0.490, reward_mean=0.4, reward_bound=0.9\n",
            "63: loss=0.390, reward_mean=0.3, reward_bound=0.0\n",
            "64: loss=0.490, reward_mean=0.3, reward_bound=0.3\n",
            "65: loss=0.427, reward_mean=0.4, reward_bound=0.9\n",
            "66: loss=0.478, reward_mean=0.3, reward_bound=0.3\n",
            "67: loss=0.409, reward_mean=0.3, reward_bound=0.9\n",
            "68: loss=0.327, reward_mean=0.4, reward_bound=1.0\n",
            "69: loss=0.303, reward_mean=0.4, reward_bound=0.9\n",
            "70: loss=0.223, reward_mean=0.3, reward_bound=0.9\n",
            "71: loss=0.330, reward_mean=0.3, reward_bound=0.7\n",
            "72: loss=0.392, reward_mean=0.4, reward_bound=0.9\n",
            "73: loss=0.339, reward_mean=0.3, reward_bound=0.8\n",
            "74: loss=0.396, reward_mean=0.3, reward_bound=0.7\n",
            "75: loss=0.360, reward_mean=0.4, reward_bound=1.0\n",
            "76: loss=0.213, reward_mean=0.4, reward_bound=0.9\n",
            "77: loss=0.322, reward_mean=0.3, reward_bound=0.8\n",
            "78: loss=0.309, reward_mean=0.4, reward_bound=1.0\n",
            "79: loss=0.314, reward_mean=0.3, reward_bound=0.5\n",
            "80: loss=0.359, reward_mean=0.3, reward_bound=0.7\n",
            "81: loss=0.547, reward_mean=0.3, reward_bound=0.0\n",
            "82: loss=0.221, reward_mean=0.4, reward_bound=1.0\n",
            "83: loss=0.294, reward_mean=0.5, reward_bound=1.0\n",
            "84: loss=0.320, reward_mean=0.4, reward_bound=1.0\n",
            "85: loss=0.173, reward_mean=0.3, reward_bound=1.0\n",
            "86: loss=0.207, reward_mean=0.4, reward_bound=0.9\n",
            "87: loss=0.282, reward_mean=0.3, reward_bound=1.0\n",
            "88: loss=0.304, reward_mean=0.3, reward_bound=0.3\n",
            "89: loss=0.177, reward_mean=0.5, reward_bound=1.0\n",
            "90: loss=0.250, reward_mean=0.5, reward_bound=1.0\n",
            "91: loss=0.379, reward_mean=0.3, reward_bound=0.8\n",
            "92: loss=0.249, reward_mean=0.4, reward_bound=1.0\n",
            "93: loss=0.203, reward_mean=0.4, reward_bound=1.0\n",
            "94: loss=0.271, reward_mean=0.3, reward_bound=0.8\n",
            "95: loss=0.236, reward_mean=0.5, reward_bound=1.0\n",
            "96: loss=0.267, reward_mean=0.4, reward_bound=1.0\n",
            "97: loss=0.172, reward_mean=0.5, reward_bound=1.0\n",
            "98: loss=0.262, reward_mean=0.4, reward_bound=1.0\n",
            "99: loss=0.296, reward_mean=0.4, reward_bound=0.9\n",
            "100: loss=0.203, reward_mean=0.5, reward_bound=1.0\n",
            "101: loss=0.178, reward_mean=0.4, reward_bound=1.0\n",
            "102: loss=0.227, reward_mean=0.4, reward_bound=1.0\n",
            "103: loss=0.164, reward_mean=0.5, reward_bound=1.0\n",
            "104: loss=0.184, reward_mean=0.5, reward_bound=1.0\n",
            "105: loss=0.215, reward_mean=0.5, reward_bound=1.0\n",
            "106: loss=0.194, reward_mean=0.6, reward_bound=1.0\n",
            "107: loss=0.180, reward_mean=0.5, reward_bound=1.0\n",
            "108: loss=0.207, reward_mean=0.5, reward_bound=1.0\n",
            "109: loss=0.220, reward_mean=0.4, reward_bound=1.0\n",
            "110: loss=0.144, reward_mean=0.6, reward_bound=1.0\n",
            "111: loss=0.165, reward_mean=0.6, reward_bound=1.0\n",
            "112: loss=0.205, reward_mean=0.5, reward_bound=1.0\n",
            "113: loss=0.150, reward_mean=0.5, reward_bound=1.0\n",
            "114: loss=0.174, reward_mean=0.5, reward_bound=1.0\n",
            "115: loss=0.153, reward_mean=0.5, reward_bound=1.0\n",
            "116: loss=0.217, reward_mean=0.6, reward_bound=1.0\n",
            "117: loss=0.126, reward_mean=0.4, reward_bound=1.0\n",
            "118: loss=0.173, reward_mean=0.6, reward_bound=1.0\n",
            "119: loss=0.150, reward_mean=0.5, reward_bound=1.0\n",
            "120: loss=0.178, reward_mean=0.6, reward_bound=1.0\n",
            "121: loss=0.115, reward_mean=0.6, reward_bound=1.0\n",
            "122: loss=0.166, reward_mean=0.6, reward_bound=1.0\n",
            "123: loss=0.142, reward_mean=0.6, reward_bound=1.0\n",
            "124: loss=0.142, reward_mean=0.5, reward_bound=1.0\n",
            "125: loss=0.118, reward_mean=0.5, reward_bound=1.0\n",
            "126: loss=0.160, reward_mean=0.5, reward_bound=1.0\n",
            "127: loss=0.163, reward_mean=0.6, reward_bound=1.0\n",
            "128: loss=0.147, reward_mean=0.6, reward_bound=1.0\n",
            "129: loss=0.146, reward_mean=0.7, reward_bound=1.0\n",
            "130: loss=0.117, reward_mean=0.8, reward_bound=1.0\n",
            "131: loss=0.154, reward_mean=0.6, reward_bound=1.0\n",
            "132: loss=0.084, reward_mean=0.6, reward_bound=1.0\n",
            "133: loss=0.158, reward_mean=0.7, reward_bound=1.0\n",
            "134: loss=0.143, reward_mean=0.7, reward_bound=1.0\n",
            "135: loss=0.154, reward_mean=0.6, reward_bound=1.0\n",
            "136: loss=0.128, reward_mean=0.6, reward_bound=1.0\n",
            "137: loss=0.208, reward_mean=0.6, reward_bound=1.0\n",
            "138: loss=0.114, reward_mean=0.7, reward_bound=1.0\n",
            "139: loss=0.122, reward_mean=0.6, reward_bound=1.0\n",
            "140: loss=0.165, reward_mean=0.6, reward_bound=1.0\n",
            "141: loss=0.171, reward_mean=0.7, reward_bound=1.0\n",
            "142: loss=0.099, reward_mean=0.7, reward_bound=1.0\n",
            "143: loss=0.130, reward_mean=0.6, reward_bound=1.0\n",
            "144: loss=0.119, reward_mean=0.7, reward_bound=1.0\n",
            "145: loss=0.109, reward_mean=0.7, reward_bound=1.0\n",
            "146: loss=0.112, reward_mean=0.6, reward_bound=1.0\n",
            "147: loss=0.114, reward_mean=0.6, reward_bound=1.0\n",
            "148: loss=0.108, reward_mean=0.6, reward_bound=1.0\n",
            "149: loss=0.111, reward_mean=0.7, reward_bound=1.0\n",
            "150: loss=0.116, reward_mean=0.6, reward_bound=1.0\n",
            "151: loss=0.117, reward_mean=0.7, reward_bound=1.0\n",
            "152: loss=0.130, reward_mean=0.6, reward_bound=1.0\n",
            "153: loss=0.136, reward_mean=0.7, reward_bound=1.0\n",
            "154: loss=0.129, reward_mean=0.7, reward_bound=1.0\n",
            "155: loss=0.124, reward_mean=0.6, reward_bound=1.0\n",
            "156: loss=0.081, reward_mean=0.8, reward_bound=1.0\n",
            "157: loss=0.122, reward_mean=0.7, reward_bound=1.0\n",
            "158: loss=0.104, reward_mean=0.7, reward_bound=1.0\n",
            "159: loss=0.158, reward_mean=0.7, reward_bound=1.0\n",
            "160: loss=0.116, reward_mean=0.8, reward_bound=1.0\n",
            "161: loss=0.118, reward_mean=0.7, reward_bound=1.0\n",
            "162: loss=0.131, reward_mean=0.7, reward_bound=1.0\n",
            "163: loss=0.120, reward_mean=0.8, reward_bound=1.0\n",
            "164: loss=0.095, reward_mean=0.8, reward_bound=1.0\n",
            "165: loss=0.136, reward_mean=0.7, reward_bound=1.0\n",
            "166: loss=0.087, reward_mean=0.8, reward_bound=1.0\n",
            "167: loss=0.107, reward_mean=0.8, reward_bound=1.0\n",
            "168: loss=0.153, reward_mean=0.7, reward_bound=1.0\n",
            "169: loss=0.085, reward_mean=0.7, reward_bound=1.0\n",
            "170: loss=0.090, reward_mean=0.8, reward_bound=1.0\n",
            "171: loss=0.079, reward_mean=0.7, reward_bound=1.0\n",
            "172: loss=0.073, reward_mean=0.8, reward_bound=1.0\n",
            "173: loss=0.087, reward_mean=0.8, reward_bound=1.0\n",
            "174: loss=0.069, reward_mean=0.8, reward_bound=1.0\n",
            "175: loss=0.083, reward_mean=0.7, reward_bound=1.0\n",
            "176: loss=0.051, reward_mean=0.9, reward_bound=1.0\n",
            "Solved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMJeC_S8zvDL",
        "outputId": "c89a59b1-acca-483c-8e79-29d01c142e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "# testing. One game\n",
        "\n",
        "from time import sleep\n",
        "from IPython.display import clear_output\n",
        "\n",
        "obs = env.reset()\n",
        "sm = nn.Softmax(dim=1)\n",
        "\n",
        "while True:\n",
        "  clear_output(wait=True)\n",
        "  sleep(1)\n",
        "\n",
        "  for i in env.state:\n",
        "    print(i)\n",
        "  print('-' * 16)\n",
        "\n",
        "  obs_v = torch.FloatTensor([obs])\n",
        "  act_probs_v = sm(net(obs_v))\n",
        "  act_probs = act_probs_v.data.numpy()[0]\n",
        "  action = np.random.choice(len(act_probs), p=act_probs)\n",
        "  next_obs, reward, is_done, _ = env.step(action)\n",
        "\n",
        "  if is_done:\n",
        "    break\n",
        "  obs = next_obs"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0.5]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0]\n",
            "----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhUiN3jRmops"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3YiANFr3csf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}